# Machine Learning

## Oxford Spring School in Advanced Research Methods 2024

This repository contains all the slides for the course. We may update these slides as we go along, so this repo will always store the latest version. 

All the `code/` required for the workshops will be uploaded a day in advance, and all complementary readings are listed below.

## Readings

### Day 1

* Grimmer, J., Roberts, M. E., & Stewart, B. M. (2021). Machine learning for social science: An agnostic approach. Annual Review of Political Science, 24, 395-419. 

* Knox, D., Lucas, C., & Cho, W. K. T. (2022). Testing causal theories with learned proxies. Annual Review of Political Science, 25, 419-441. 

* Spirling, A. & Stewart, B. M. (2022). What good is a regression? Inference to the best explanation and the practise of political science research. Working paper. 

### Day 2

* Kleinberg, J., Ludwig, J., Mullainathan, S., & Obermeyer, Z. (2015). Prediction policy problems. American Economic Review, 105(5), 491-495. 

* Kim, I. S. (2017). Political cleavages within industry: Firm-level lobbying for trade liberalization. American Political Science Review, 111(1), 1-20. 

* Blackwell, M., & Olson, M. P. (2022). Reducing model misspecification and bias in the estimation of interactions. Political Analysis, 30(4), 495-514. 

### Day 3

* Montgomery, J. M., & Olivella, S. (2018). Tree‚ÄêBased Models for Political Science Data. American Journal of Political Science, 62(3), 729-744. 

* Hill, J. L. (2011). Bayesian nonparametric modeling for causal inference. Journal of Computational and Graphical Statistics, 20(1), 217-240. 

* Athey, S., & Wager, S. (2019). Estimating treatment effects with causal forests: An application. Observational studies, 5(2), 37-51. 

### Day 4

* Lall, R., & Robinson, T. (2022). The MIDAS touch: accurate and scalable missing-data imputation with deep learning. Political Analysis, 30(2), 179-196. 

* Bellodi, L. (2022). A dynamic measure of bureaucratic reputation: New data for new theory. American Journal of Political Science. 

* Gal, Y., & Ghahramani, Z. (2016, June). Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In international conference on machine learning (pp. 1050-1059). PMLR. 

### Day 5

* Grimmer, J., Messing, S., & Westwood, S. J. (2017). Estimating heterogeneous treatment effects and the effects of heterogeneous treatments with ensemble methods. Political Analysis, 25(4), 413-434. 

* Hare, C., & Kutsuris, M. (2023). Measuring swing voters with a supervised machine learning ensemble. Political Analysis, 31(4), 537-553. 