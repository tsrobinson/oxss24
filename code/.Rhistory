cces <- read.csv("../data/cces_formatted_oxss.csv")
library(tidyverse)
library(haven)
library(randomForest)
library(BART)
library(dplyr)
cces <- read.csv("../data/cces_formatted_oxss.csv")
# Convert predictors to factors
for (v in 1:ncol(cces)) {
if (is.character(cces[[v]])) {
cces[[v]] <- as.factor(cces[[v]])
}
}
View(cces)
ncol(cces)
# How about the effective number of columns?
unique_cols <- apply(
cces,2, function (x) ifelse(is.numeric(x),1,length(unique(x)))
)
unique_cols
sum(unique_cols)
cces$vote2016
cces$votetrump <- ifelse(cces$vote2016 == "Donald Trump", "Trump","Other")
cces$votetrump <- as.factor(cces$votetrump)
cces$vote2016 <- NULL
set.seed(89)
train_indices <- sample(1:nrow(cces), 0.666*nrow(cces))
train_indices
test_indices <-  setdiff(1:nrow(cces), train_indices)
X_train <- cces[train_indices, ]
X_test <- cces[test_indices,] %>% select(-votetrump)
Y_test <- cces$votetrump[test_indices]
prop.table(table(X_train$votetrump))
prop.table(table(Y_test))
rf_model <- randomForest(votetrump ~ ., data = X_train,
mtry = 2, ntree = 500,
importance = TRUE)
importance(rf_model)
varImpPlot(rf_model)
rf_predictions <- predict(rf_model, X_test, type = "class")
rf_predictions
rf_acc <- mean(rf_predictions == Y_test)
print(paste0("Random Forest accuracy: ", round(rf_acc,3)))
rf_comp <- data.frame(y_pred = rf_predictions,
y_true = Y_test)
k_folds <- sample(1:10, nrow(X_train), replace = TRUE)
k_folds
sample(1:10, nrow(X_train), replace = TRUE)
k_folds <- sample(1:10, nrow(X_train), replace = TRUE)
gc()
# NB: The BART package requires a numeric outcome variable
bart_model <- pbart(x.train = X_train[,names(X_train) != "votetrump"],
y.train = ifelse(X_train$votetrump == "Trump",1,0),
ntree = 50L, numcut = 100L)
bart_pred_probs <- predict(bart_model, newdata = bartModelMatrix(X_test))
View(bart_pred_probs)
bart_pred_probs$prob.test.mean
round(bart_pred_probs$prob.test.mean)
bart_pred_bin <- round(bart_pred_probs$prob.test.mean)
# And relabel
bart_predictions <- ifelse(bart_pred_bin == 1, "Trump","Other")
bart_acc <- mean(bart_predictions == Y_test)
print(paste0("BART accuracy: ", round(bart_acc,3)))
full_comp <- data.frame(y_true = Y_test,
y_rf = rf_predictions,
y_bart = bart_predictions)
View(full_comp)
rf_predictions[373]
rf_probs <- predict(rf_model, X_test, type = "response")
rf_probs
?predict.randomForest
rf_probs <- predict(rf_model, X_test, type = "prob")
rf_probs[373]
bart_pred_probs[373]
bart_pred_probs$prob.test.mean[373]
library(tensorflow)
library(keras)
library(tidyverse)
library(recipes)
## Read in the data
adult <- read_csv("https://raw.githubusercontent.com/MIDASverse/MIDASpy/master/Examples/adult_data.csv") %>%
drop_na() %>% # Not good practise!! (for the sake of demonstration only)
select(-1) # remove the first column as it's just the row indices
# Break up our data into train and test
train_index <- sample(1:nrow(adult), 0.666*nrow(adult), replace = FALSE)
adult_train <- adult[train_index,]
adult_test <- adult[setdiff(1:nrow(adult), train_index),]
y_train <- ifelse(adult_train$class_labels == ">50K",1,0)
y_test <- ifelse(adult_test$class_labels == ">50K",1,0)
# Construct a "recipe"
rec_obj <- recipe(class_labels ~ ., data = adult) %>%
step_dummy(all_nominal(), -all_outcomes()) %>% # One-hot encode columns
step_center(all_predictors(), -all_outcomes()) %>% # Centre all predictors on 0
step_scale(all_predictors(), -all_outcomes()) %>% # Scale all predictors with sd=1
prep(data = adult)
x_train <- bake(rec_obj, new_data = adult_train) %>% select(-class_labels)
x_test  <- bake(rec_obj, new_data = adult_test) %>% select(-class_labels)
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
library(tensorflow)
library(keras)
library(tidyverse)
adult <- read_csv("https://raw.githubusercontent.com/MIDASverse/MIDASpy/master/Examples/adult_data.csv") %>%
drop_na() %>% # Not good practise!! (for the sake of demonstration only)
select(-1)
View(adult)
train_index <- sample(1:nrow(adult), 0.666*nrow(adult), replace = FALSE)
adult_train <- adult[train_index,]
adult_test <- adult[setdiff(1:nrow(adult), train_index),]
y_train <- ifelse(adult_train$class_labels == ">50K",1,0)
y_test <- ifelse(adult_test$class_labels == ">50K",1,0)
recipe(class_labels ~ ., data = adult)
rec_obj <- recipe(class_labels ~ ., data = adult) %>%
step_dummy(all_nominal(), -all_outcomes()) %>% # One-hot encode columns
step_center(all_predictors(), -all_outcomes()) %>% # Centre all predictors on 0
step_scale(all_predictors(), -all_outcomes()) %>% # Scale all predictors with sd=1
prep(data = adult)
x_train <- bake(rec_obj, new_data = adult_train) %>% select(-class_labels)
x_test  <- bake(rec_obj, new_data = adult_test) %>% select(-class_labels)
View(x_train)
model <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
# once defined, we then compile this network
compile(
optimizer = 'sgd', # Stochastic gradient descent -- what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
history <- fit(
object = model,
x = as.matrix(x_train),
y = y_train,
batch_size = 50,
epochs = 50,
validation_split = 0.30
)
model_w_dropout <- keras_model_sequential() %>%
layer_dense(units = 32, activation = 'relu', input_shape = ncol(x_train)) %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 16, activation = 'relu') %>%
layer_dropout(rate = 0.5) %>%
layer_dense(units = 1, activation = 'sigmoid') %>%
compile(
optimizer = 'sgd', # Stochastic gradient descent -- a variation of what we hand-coded on Monday!
loss      = 'binary_crossentropy',
metrics   = c('accuracy') # Determines what is plotted while training occurs
)
history2 <- fit(
object = model_w_dropout,
x = as.matrix(x_train),
y = y_train,
batch_size = 50,
epochs = 50,
validation_split = 0.30
)
